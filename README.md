# DE-training-101

## Introduction
This repository focuses on how to process data using PySpark and Airflow. 

## Data Lake

Amazon S3 is chosen as the data lake.

## Infrastructure

The infrastructure part is omitted.

## set up aws env
1. run `aws configure sso`, set  `SSO start URL` and `SSO Region`, replace your profile name in `extract_data`